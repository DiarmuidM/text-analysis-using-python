{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we introduce and apply a range of supervised text analysis techniques to social science data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use Python to analyse text data relating to charitable activities.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data preprocessing problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 40-60 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand and apply common supervised text analysis techniques to social science data.\n",
    "    3. Be able to use Python for performing text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*How do we analyse social science text data?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we analyse social science text data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a wide array of text analysis techniques that we could apply in our research:\n",
    "* **Descriptive inference:** how to characterise text; vector space model, bag of words, (dis)similarity measures, diversity, complexity, style, bursts.\n",
    "* **Supervised techniques:** dictionaries, sentiment analysis, categorising.\n",
    "* **Unsupervised techniques:** cluster analysis, Principal Components Analysis (PCA), topic modelling, embeddings. (Spirling, 2022)\n",
    "\n",
    "To say nothing of using Generative AI or Large Language Models (LLMs) to conduct these analyses on our behalf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we focus on two common supervised text analysis techniques:\n",
    "* Keyword searching / KWIC\n",
    "* Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First we need to ensure Python has the functionality it needs for text analysis. As you will see, it needs quite a bit of extra functionality, so this may take some time to install / import depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages - only run once per machine\n",
    "!pip install textblob\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for general data and file management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for processing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words     # list of valid words\n",
    "english_words = set(words.words())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import wordnet                    # Functions we need for lemmatising\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer         # Functions we need for stemming\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for analysing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# for data visualisation\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # for data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second important preliminary step is to import the text data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"https://raw.githubusercontent.com/SGSSSonline/text-analysis-summer-school-2025/refs/heads/main/data/acnc-overseas-activities-2022.csv\" # define file to be imported\n",
    "\n",
    "data = pd.read_csv(infile, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_desc\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Create Document Term Matrix\n",
    "\n",
    "You have likely created and saved this in a previous lesson but let's start afresh just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text and convert to lowercase\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    #print(lower_words)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    a_words = [word for word in lower_words if word.isalpha()]\n",
    "    #print(\"Alpha words: \",a_words)\n",
    "\n",
    "    # Lemmatise words\n",
    "    lemmed_words = [lemmatizer.lemmatize(word) for word in a_words]\n",
    "    #print(\"Lemmed words: \",lemmed_words)\n",
    "    \n",
    "    # Remove non-English words\n",
    "    e_words = [word for word in lemmed_words if word in english_words]\n",
    "    #print(\"English words: \", e_words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stop_words = [\"registered\", \"registration\", \"company\", \"number\", \"australia\", \n",
    "                      \"australian\", \"report\", \"charity\", \"charities\", \"charitable\", \"year\", \n",
    "                      \"end\", \"statement\", \"statements\", \"trustee\", \"trustees\", \"trust\", \"overseas\",\n",
    "                     \"international\", \"support\", \"fund\", \"provide\", \"provision\", \"activity\", \"activities\",\n",
    "                     \"providing\", \"provided\", \"program\", \"programme\", \"project\"]\n",
    "    stop_words.update(new_stop_words)\n",
    "    s_words = [word for word in e_words if word not in stop_words]\n",
    "    #print(\"Stop words: \", s_words)\n",
    "\n",
    "    # Stem words\n",
    "    #stemmed_words = [porter.stem(word) for word in p_words]\n",
    "\n",
    "    # Remove words with less than three characters\n",
    "    clean_words = [word for word in s_words if len(str(word)) > 2]\n",
    "\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is valid\n",
    "data[\"activity_desc\"] = data[\"activity_desc\"].astype(str)\n",
    "data = data.dropna(subset=[\"activity_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"activity_desc\"].apply(preprocess_text)\n",
    "data[[\"abn\", \"activity_desc\", \"clean_text\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to loop over every row in the dataset and extract the charity unique id and the cleaned activity description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(row[\"abn\"], row[\"clean_text\"]) for _, row in data.iterrows()]\n",
    "documents[0:5] # view first five elements in list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the cleaned text for converting to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [text for _, text in documents]\n",
    "text_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Document-Term Matrix using a Count or TF-IDF vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\r",
    "bow = vectorizer.fit_transform(text_data)\n",
    "terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()\r",
    "#bow = vectorizer.fit_transform(text_data)\n",
    "#terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM into a Pandas DataFrame\n",
    "dtm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out(), index=[doc_id for doc_id, _ in documents])\n",
    "document_ids = dtm.index.tolist() # create list of document ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', None) # change display options\n",
    "pd.set_option('display.max_columns', None) # change display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms[0:500]) # view first 500 terms in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A supervised text analysis technique (or supervised learning more generally) is one that seeks to understand the relationship between a set of features (e.g., document word counts) and an outcome (e.g., what category or class a document belongs to). Logistic regression is a type of supervised learning technique as it estimates the relationship between a set of covariates and the probability of an observation being in a given category or not. This and similar techniques are termed **supervised** because the category or class is already known: we know whether a document was written by a small or large charity in our example. Therefore we are \"supervising\" the classifier / algorithm as it seeks to understand the relationship between the features and the outcome.\n",
    "\n",
    "Often the outcome is known because it is a feature in the data e.g., in addition to charities' activity descriptions we have their organisation size and other characteristics. Other times we need to label the data so that the outcome is known: for example, we may manually review a sample of documents and categorise them as either \"Good practice\" or \"Not good practice\" (or whatever classification scheme we are using)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a well established and commonly used supervised learning technique for text data. The core idea is to use a set of pre-defined words with specific connotations to classify our documents automatically, quickly and accurately (Spirling, 2022). This set of pre-defined words, or **dictionary**, serves as our labelled data: we know whether the word \"amazing\" is positive or not, we just need to access this information from a suitable dictionary. Although in your data the outcome is unknown - whether a piece of text has a positive or negative sentiment or tone - , the outcome is known in general as we can just look up the words in your data and see their sentiment score. Then it begins a simple calculation to see how positive or negative the words in a piece of text / document are overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple piece of text to demonstrate the core idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Pogues’ Body of an American is a raw, energetic anthem. The driving rhythm, powerful lyrics, and Shane MacGowan’s \n",
    "passionate vocals make it an unforgettable and emotional listening experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "sentiment_score = blob.sentiment.polarity\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a lot of code for once, that's nice. We pass the text to a functiot called `TextBlob` which looks up the dictionary of words and their sentiment scores. This returns an object called `blob` (call it something else if you like) which we can this access its attributes. One of those is the overall sentiment score, which in this case is *.19*. Scores can range from -1 (very negative) to +1 (very positive). Scores near zero represent text that is neither positive or negative overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Using the `sentiment_score` how would you describe the sentiment of the song review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are generally interested in the overall sentiment of the piece of text but we can also access the sentiment scores for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentiments = {word: TextBlob(word).sentiment.polarity for word in blob.words}\n",
    "word_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What do you notice about how the overall sentiment score was constructed from individual word scores? Do you agree with the sentiment scores for individual scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Take one of the short reviews for the movie \"Io Capitano\" and produce a sentiment score (overall and for each word). Do you agree with the results and do they fit the original text of the review? https://www.rottentomatoes.com/m/io_capitano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's apply sentiment analysis to our charity activity data. First let's create a function that can be applied to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment_score\"] = data[\"clean_text\"].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a subset of columns in the dataset\n",
    "data[[\"abn\", \"activity_desc\", \"clean_text\", \"sentiment_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be a bit difficult to view the whole dataset in Python / Jupyter Notebook, so let's try a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [(row[\"abn\"], row[\"activity_desc\"], row[\"sentiment_score\"]) for _, row in data.iterrows()]\n",
    "sentiments[0:20] # view first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What do you think of the sentiment scores for the first 20 documents in the corpus? Do you agree with the scores? Is it substantively meaningful to conceptualise these activity descriptions as positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment score is a numeric representation of document tone, so let's look at the distribution of these scores across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment_score\"].hist(bins=20, edgecolor='black', figsize=(8, 5))\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of document sentiment scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** How would you characterise the sentiment / tone of charity activity descriptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's disaggregate sentiment scores by charity size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of sentiment score by charity size \n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=\"charitysize\", y=\"sentiment_score\", data=data)\n",
    "plt.xlabel(\"Charity Size\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.title(\"Distribution of sentiment score by charity size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Are there meaningful differences in sentiment score by charity size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=data, x=\"sentiment_score\", hue=\"charitysize\", common_norm=False, fill=True, alpha=0.3)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Kernel Density Plot of Sentiment Scores by Charity Size\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for text analysis tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to perform supervised text analyses**. There are a number of common and key analytical techniques that can yield substantive insight into key features of documents.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "These are but a selection of the analytical techniques at your displosal; however they are common and often key ones in text analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform sentiment analysis using the other file in the data folder (*acnc-overseas-activities-2021.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
