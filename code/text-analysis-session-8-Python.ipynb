{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we introduce and apply a range of unsupervised text analysis techniques to social science data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use Python to analyse text data relating to charitable activities.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data preprocessing problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 60-80 minutes\n",
    "* **Pre-requisites**: Practical 1, Practical 2\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand and apply common unsupervised text analysis techniques to social science data.\n",
    "    3. Be able to use Python for performing text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*How do we prepare social science data for text analysis?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we analyse social science text data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a wide array of text analysis techniques that we could apply in our research:\n",
    "* **Descriptive inference:** how to characterise text; vector space model, bag of words, (dis)similarity measures, diversity, complexity, style, bursts.\n",
    "* **Supervised techniques:** dictionaries, sentiment analysis, categorising.\n",
    "* **Unsupervised techniques:** cluster analysis, Principal Components Analysis (PCA), topic modelling, embeddings. (Spirling, 2022)\n",
    "\n",
    "To say nothing of using Generative AI or Large Language Models (LLMs) to conduct these analyses on our behalf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we focus on a number of common unsupervised text analysis techniques:\n",
    "* Principal Components Analysis (PCA)\n",
    "* Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First we need to ensure Python has the functionality it needs for text analysis. As you will see, it needs quite a bit of extra functionality, so this may take some time to install / import depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages - only run once per machine\n",
    "!pip install seaborn\n",
    "!pip install pyldavis\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for general data and file management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for processing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)  \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words     # list of valid words\n",
    "english_words = set(words.words())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import wordnet                    # Functions we need for lemmatising\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer         # Functions we need for stemming\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for analysing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data visualisation\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # for data visualisation\n",
    "\n",
    "# for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for topic modelling\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# for topic modelling evaluation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second important preliminary step is to import the text data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"https://raw.githubusercontent.com/SGSSSonline/text-analysis/refs/heads/main/data/acnc-overseas-activities-2022.csv\" # define file to be imported\n",
    "\n",
    "data = pd.read_csv(infile, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_desc\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Create Document Term Matrix\n",
    "\n",
    "You have likely created and saved this in a previous lesson but let's start afresh just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text and convert to lowercase\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    #print(lower_words)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    a_words = [word for word in lower_words if word.isalpha()]\n",
    "    #print(\"Alpha words: \",a_words)\n",
    "\n",
    "    # Lemmatise words\n",
    "    lemmed_words = [lemmatizer.lemmatize(word) for word in a_words]\n",
    "    #print(\"Lemmed words: \",lemmed_words)\n",
    "    \n",
    "    # Remove non-English words\n",
    "    e_words = [word for word in lemmed_words if word in english_words]\n",
    "    #print(\"English words: \", e_words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stop_words = [\"registered\", \"registration\", \"company\", \"number\", \"australia\", \n",
    "                      \"australian\", \"report\", \"charity\", \"charities\", \"charitable\", \"year\", \n",
    "                      \"end\", \"statement\", \"statements\", \"trustee\", \"trustees\", \"trust\", \"overseas\",\n",
    "                     \"international\", \"support\", \"fund\", \"provide\", \"provision\", \"activity\", \"activities\",\n",
    "                     \"providing\", \"provided\", \"program\", \"programme\", \"project\"]\n",
    "    stop_words.update(new_stop_words)\n",
    "    s_words = [word for word in e_words if word not in stop_words]\n",
    "    #print(\"Stop words: \", s_words)\n",
    "\n",
    "    # Stem words\n",
    "    #stemmed_words = [porter.stem(word) for word in p_words]\n",
    "\n",
    "    # Remove words with less than three characters\n",
    "    clean_words = [word for word in s_words if len(str(word)) > 2]\n",
    "\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is valid\n",
    "data[\"activity_desc\"] = data[\"activity_desc\"].astype(str)\n",
    "data = data.dropna(subset=[\"activity_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"activity_desc\"].apply(preprocess_text)\n",
    "data[[\"abn\", \"activity_desc\", \"clean_text\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to loop over every row in the dataset and extract the charity unique id and the cleaned activity description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(row[\"abn\"], row[\"clean_text\"]) for _, row in data.iterrows()]\n",
    "documents[0:5] # view first five elements in list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the cleaned text for converting to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [text for _, text in documents]\n",
    "text_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Document-Term Matrix using a Count or TF-IDF vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\r",
    "#bow = vectorizer.fit_transform(text_data)\n",
    "#terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\r",
    "bow = vectorizer.fit_transform(text_data)\n",
    "terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM into a Pandas DataFrame\n",
    "dtm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out(), index=[doc_id for doc_id, _ in documents])\n",
    "document_ids = dtm.index.tolist() # create list of document ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', None) # change display options\n",
    "pd.set_option('display.max_columns', None) # change display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms[0:500]) # view first 500 terms in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unsupervised text analysis technique (or unsupervised learning more generally) is one that seeks to uncover or determine what category or class a document belongs to. Cluster analysis is a type of unsupervised learning technique as it groups observations according to shared characteristics or features. We do not know *a priori* what group a document belongs to, we need to estimate or predict based on the features of the document. This and similar techniques (e.g., PCA, topic modelling) are termed **unsupervised** because the category or class is not already known and the text analysis technique is therefore not guided or supervised as to what the correct categories or classes are.\n",
    "\n",
    "Perhaps documents share a linguistic style, or talk about similar topics at similar rates. Unless we want to read a lot of these documents and manually code them into particular categories or classes, we need to use techniques that construct these groups from the ground up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is an unsupervised technique that produces a lower dimensional 'map' or projection of our documents in a new space. That is, there is a latent group or structure ('components') to the documents that we want to estimate; we can then visualise how our documents map to these components and attempt to interpret their substantive meaning.\n",
    "\n",
    "PCA can be a bit confusing at first and is applicable to many other settings besides text analysis, with terms we may not be familiar with ('loadings', 'factor scores', 'eigenvalues'), so let's state some terminology and definitions to help us with our understanding:\n",
    "* Principal components (PCs) are combinations of original features in the data. In text analysis, this may be groupings of documents based on how similar their word use is.\n",
    "* Each observation maps or projects onto each PC through its factor score. In text analysis, documents will be particularly associated with certain groupings and not associated with others.\n",
    "* Each original feature in the data maps onto each PC through its loading score. In text analysis, each word in the corpus of documents will be particularly associated (or not) with certain groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully PCA is simple to implement using the DTM, so let's solidify our understanding using our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5) # estimate 5 PCs\n",
    "principal_components = pca.fit_transform(dtm) # extract PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components # view PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reason PCA uses terms like 'map' and 'project': without visualisations the results can be very difficult to interpret. First let's convert to a dataframe and then visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, now we can see how each document maps onto each of the five PCs through its factor scores. For example, document 1 (index number 0) seems to be particularly associated with the first PC and not with the other four. Somewhat confusingly, we are not interested in whether a value is positive or negative (its sign), just how large it is (its magnitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a couple of decisions to make regarding our analysis:\n",
    "1. How many PCs do we keep?\n",
    "2. How do we interpret them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the first question, we need to examine how much variance is explained by each PC. The variance is a summary of all the differences between the documents in terms of their term frequencies (the DTM). Remember our aim with PCA is to reduce the documents (i.e., the DTM) to a lower dimension representation. So instead of having lots of columns in the DTM representing each unique term in the corpus, why don't we have five columns that represent combinations of the unique terms and thus reveal some latent structure? But we just picked five as a starting point: perhaps having three PCs explains nearly as much of the variance as having five? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot eigenvalues (Scree Plot)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "components = range(1, len(eigenvalues) + 1)\n",
    "plt.bar(components, eigenvalues, color='skyblue')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Eigenvalues of Principal Components')\n",
    "plt.xticks(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance: individual and cumulative\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(components, explained_variance_ratio, alpha=0.5, align='center',\n",
    "        label='Individual explained variance')\n",
    "plt.step(components, explained_variance_ratio.cumsum(), where='mid', \n",
    "         label='Cumulative explained variance', color='red')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xticks(components)\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the above results. Starting with the first plot, the larger the eigenvalue for a PC, the more important it is for explaining the variance in the DTM. We see an obvious pattern: the first PC is clearly most important for explaining the variance, while PCs 2 - 5 are quite similar in the terms of their contribution to explaining the variance. \n",
    "\n",
    "The second plot is a different way of showing the same thing: how much of the variance is explained by each component and by the five components overall. In terms of the latter, the five components explain roughly 12% of the variance in the documents. Is this enough? It really depends on your research question and aims: in this example we know overseas charities write quite different text when describing their activities, so should we be surprised that having five groupings of these documents does not account for very much variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Change the number of principal components in the `pca = PCA(n_components=5)`code block above and rerun the analysis. Interpret the results: have you explained more or less of the variance? Is this better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's assume five PCs is suitable for our purposes. How do we interpret what the PCs actually represent? What combinations of terms make up these PCs, and therefore are they substatively interesting or conceptually important? To do this we need to look at each component and find the words most strongly associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_ # extract loadings for each term to the PCs\n",
    "n_components = 5 # number of PCs you estimated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top ten terms for each principal component\n",
    "fig, axes = plt.subplots(n_components, 1, figsize=(10, 4*n_components))\n",
    "\n",
    "for i in range(n_components):\n",
    "    # Get the loadings for component i\n",
    "    component_loadings = loadings[i]\n",
    "    # Compute absolute loadings for ranking\n",
    "    abs_loadings = np.abs(component_loadings)\n",
    "    # Get indices of top ten terms\n",
    "    top_indices = np.argsort(abs_loadings)[-10:][::-1]\n",
    "    top_terms = [terms[idx] for idx in top_indices]\n",
    "    top_values = component_loadings[top_indices]\n",
    "    \n",
    "    # Create a bar plot for the component\n",
    "    axes[i].bar(top_terms, top_values, color='skyblue')\n",
    "    axes[i].set_title(f'Component {i+1} Top 10 Term Loadings')\n",
    "    axes[i].set_ylabel('Loading Value')\n",
    "    axes[i].set_xlabel('Term')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models are an important intellectual development in the social sciences (Blei, 2012). Topic modelling is a technique for discovering the main themes in a corpus of documents. At its simplest these topics can be used to organise the documents in a corpus; that is, are there groups of documents talking about similar topics? However topic models can also be used to measure and explain the prevalence of interesting themes across documents e.g., do small charities talk about a certain topic more frequently than medium and large organisations?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get straight into this technique using the full charity activity DTM. We will use a package we haven't seen before (`gensim`) so don't worry if the code is unfamiliar: we just need a couple of simple steps to convert our DTM to a format that package can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[word for word, freq in zip(dtm.columns, row) for _ in range(freq)] for row in dtm.to_numpy()]\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below = 10, no_above= .95) # words must appear in at least 10 documents, and no more than 95% of documents\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dictionary size after filtering: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models can take quite long to run depending on the size of the corpus, how many topics we are looking for, how many passes through the corpus we take etc. Therefore it can help if we limit the scope of the topic modelling. We do this in the above code block by only including words that appear in at least 10 documents and in no more than 95% of documents.\n",
    "\n",
    "Notice that the filtering of very rare and very common terms has reduced the number of unique terms from c.4,200 to c.800. This may not be optimal but is a good starting point for our first topic model. (If you want to avoid any filtering, just place a '#' at the beginning of `dictionary.filter_extremes(no_below = 10, no_above= .95)` above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's estimate a topic model with five topics, and let's have the model pass over each document 10 times, and the corpus as a whole 10 times. (Think of this as a human reading each document ten times and then going over the whole corpus ten times before determining what the five topics are.) In addition we set a seed, which is a way of ensuring the topic model generates the same results every time. Remember that topic models are **probabilistic** by design: they estimate the distribution of documents over topics, and topics over words. These distributions change slightly every time you estimate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 37\n",
    "topic_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, iterations=10, passes=10, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error messages means the code ran successfully but what about the results? First let's extract the topics and the top 20 words associated with each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "for topic_id in range(topic_model.num_topics):\n",
    "    top_words = topic_model.show_topic(topic_id, topn=20)\n",
    "    topics[f\"Topic {topic_id+1}\"] = [word for word, _ in top_words]\n",
    "\n",
    "# Convert topics to DataFrame for display\n",
    "topics_df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the proportion of a topic that is constituted by each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_model.print_topics(num_topics=5, num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Increase the number of words (`num_words=10`) in the above code to see what proportion other words contribute to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the number of unique terms associated with each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts = {f\"Topic {i+1}\": len(topic_model.show_topic(i, topn=len(dictionary))) \n",
    "                     for i in range(topic_model.num_topics)}\n",
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like every term is associated with every topic, which is not informative. However this is expected: all documents share the same topics, and all topics share the same words. Where topics differ is in the probabilties each word has with each topic. Put another way, what is the contribution of each term to a topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts = {\n",
    "    f\"Topic {i+1}\": sum(1 for _, prob in topic_model.show_topic(i, topn=len(dictionary)) if prob > 0.01) \n",
    "    for i in range(topic_model.num_topics)\n",
    "}\n",
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there are only a small number of unique terms per topic that make up a meaningful proportion of the topic (i.e., more than 1% probability of being associated with a topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular representations of the topics can be informative but visualisations are an excellent way of deriving deeper insight from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare visualization\n",
    "tm = gensimvis.prepare(topic_model, corpus, dictionary, mds='mmds', sort_topics=True)\n",
    "\n",
    "# Display visualization\n",
    "pyLDAvis.display(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation can be overwhelming so here is some practical guidance for interpreting the findings:\n",
    "* Look at the left panel (scatterplot of topics): Are topics well-separated, or do they overlap?\n",
    "* Click on topics one by one: What are the key words in each topic?\n",
    "* Adjust the λ slider: Does it help clarify the topics? Which words are exclusive to a particular topic (choose a small lambda value)?\n",
    "* Compare the top words: Do they align with expected themes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial interpretation of topic 4 suggests it is about religion. Words like \"church\", \"ministry\", and \"faith\" are both common to and highly associated with this topic: that is, these words appear frequently in relation to this topic **and** when these words are used in the corpus, it is almost entirely in relation to this topic (adjust the λ slider to a lower value to see words that are not that common in the corpus overall but are to a given topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Select topic 5 and interpret the results. What theme do you think this topic represents? What words are common to this topic in terms of freqeuncy and proportion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovering the Document-Topic Matrix and the Topic-Term Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling produces two sets of results or matrices of interest:\n",
    "* The Document-Topic Matrix - the distribution of documents (rows) across topics (columns). The cells contain the probability of each topic appearing in a document.\n",
    "* The Topic-Term Matrix - the distribution of topics (rows) across terms (columns). The cells contain the probability of each word appearing in a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Topic Matrix\n",
    "doc_topic_matrix = []\n",
    "for doc_bow in corpus:\n",
    "    topic_distribution = topic_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    doc_topic_matrix.append([prob for _, prob in topic_distribution])\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "doc_topic_df = pd.DataFrame(doc_topic_matrix, columns=[f\"Topic {i+1}\" for i in range(topic_model.num_topics)])\n",
    "\n",
    "# Add original document ids\n",
    "doc_topic_df[\"doc_id\"] = document_ids # this is possible because topic modelling preserves row order from the DTM\n",
    "\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we validate whether the documents actually cohere with the topics? We have interpreted topic 4 as 'religion' though there are other words relating to education. Let's look at the documents that map most closely to topic 4 and read the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.sort_values(by='Topic 4', ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, document `87161085650` seems to be mainly about topic 4 ('religion'), as evidence by its high proportion (97% of the document's words are associated with this topic). Let's perform a close reading of this document to see if this really is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show full content in columns, including long text\n",
    "data.loc[data[\"abn\"] == 87161085650, [\"abn\", \"activity_desc\", \"clean_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, seems to be more about education than religion. Let's look at some others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"abn\"] == 76611738464, [\"abn\", \"activity_desc\", \"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth') # reset display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Term Matrix\n",
    "topic_term_matrix = topic_model.get_topics()\n",
    "topic_term_df = pd.DataFrame(topic_term_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\n",
    "topic_term_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know how many topics is optimal? Well this is really a subjective task, as it is the analyst's judgement that matters most here. There are \"objective\" approaches we could take. One of these is called the *coherence score*: this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model = CoherenceModel(model=topic_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherence score: \", coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for text analysis tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to perform unsupervised text analyses**. There are a number of common and key analytical techniques that can yield substantive insight into key features of documents.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "These are but a selection of the analytical techniques at your displosal; however they are common and often key ones in text analysis projects. Topic modelling in particular is a comprehensive analytical technique that deserves deeper engagement and practice with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform topic modelling using the other file in the data folder (*acnc-overseas-activities-2021.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
